
ğŸŒ¾ **KCC Query Assistant**

An offlineâ€‘capable, localâ€‘first AI assistant for answering agricultural queries using the Kisan Call Center (KCC) dataset with RAG (Retrievalâ€‘Augmented Generation) and a local LLM (via Ollama).

It works completely offline after setup:

Cleans and chunks the KCC dataset

Embeds documents using paraphrase-multilingual-mpnet-base-v2

Stores embeddings in ChromaDB

Retrieves relevant context for a query

Generates answers with Ollama (e.g., Gemma, Mistral)

âœ¨ **Features**
Offlineâ€‘capable â€” Runs fully on your machine

Multilingual â€” Supports Indian languages for queries and data

RAG Pipeline â€” Combines semantic search + LLM for better answers

Fallback â€” Warns when no relevant local context is found

Web UI â€” Streamlit interface for easy querying

ğŸ“‚ **Project Structure**
```bash
KCCQueryAssistant/
â”œâ”€â”€ data/                   # Raw & preprocessed dataset
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ preprocessed/
â”œâ”€â”€ vectorstore/            # ChromaDB persistent storage
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess.py       # Clean & chunk KCC data
â”‚   â”œâ”€â”€ embed_and_store.py  # Generate embeddings & store in ChromaDB
â”‚   â”œâ”€â”€ rag_core.py         # Core RAG logic (retrieval + LLM)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py              # Streamlit frontend
â”œâ”€â”€ Dockerfile              # App container
â”œâ”€â”€ docker-compose.yml      # Ollama + App services
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

ğŸ› ï¸ **Prerequisites**
Docker & Docker Compose

At least 8GB RAM for embeddings + small LLMs

(Optional) Python 3.10+ for running without Docker

ğŸš€ **Quick Start (with Docker)**

***1. Create a repository***
mkdir KCCQueryAssistant
***2. Place the KCC dataset***
Put the raw KCC CSV inside:
data/raw/
***3. Build docker image***
bash build_docker.sh

docker compose up
This will start:

Ollama on http://localhost:11434

Streamlit UI on http://localhost:8501

***4. Pull an LLM model***
Once Ollama is running:

docker exec -it ollama ollama pull gemma
(You can also use mistral or llama3)

***5. Preprocess & embed data***
Run these inside the container:

docker exec -it kcc-query-assistant python scripts/preprocess.py
docker exec -it kcc-query-assistant python scripts/embed_and_store.py
***6. Access the Web UI***
Go to: http://localhost:8501

**Ask queries like:**

"What pest-control methods are recommended for paddy in Tamil Nadu?"

"How to manage drought stress in groundnut cultivation?"

"Common sugarcane diseases in Maharashtra?"

âš™ï¸ ***Environment Variables***
Variable	Default	Description
PYTHONPATH	/app	Project root for imports

ğŸ–¥ï¸ **Running Without Docker**
If you donâ€™t want Docker:

python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
pip install -r requirements.txt
Run Ollama locally (host machine):

ollama serve
ollama pull gemma
Preprocess & embed:

python scripts/preprocess.py
python scripts/embed_and_store.py
Run the Streamlit UI:


streamlit run ui/app.py
